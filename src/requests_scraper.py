"""
Requestsæ•°æ®çˆ¬å–æ¨¡å—
ä¸“é—¨è´Ÿè´£ä½¿ç”¨è®¤è¯åŒ…è¿›è¡Œé«˜æ•ˆçš„æ•°æ®çˆ¬å–
æŒ‰ç…§READMEè¦æ±‚ï¼ŒRequestsæ¥ç®¡æ‰€æœ‰æ•°æ®æ‹‰å–ä»»åŠ¡
"""

import time
import random
import requests
from typing import Dict, List, Optional
from loguru import logger

from .config import *


class RequestsDataScraper:
    """Requestsæ•°æ®çˆ¬å–å™¨ - ä¸“é—¨è´Ÿè´£æ•°æ®æ‹‰å–"""
    
    def __init__(self, auth_package: Dict):
        """
        åˆå§‹åŒ–Requestsçˆ¬å–å™¨
        
        Args:
            auth_package: åŒ…å«headerså’Œcookiesçš„è®¤è¯åŒ…
        """
        self.auth_package = auth_package
        self.session = requests.Session()
        self._setup_session()
        
    def _setup_session(self):
        """
        è®¾ç½®Sessionçš„headerså’Œcookies
        æŒ‰ç…§READMEè¦æ±‚ç²¾ç¡®æ„é€ è¯·æ±‚
        """
        try:
            logger.info("ğŸ”§ å¼€å§‹è®¾ç½®Requests Session")
            
            # 1. è®¾ç½®è®¤è¯headersï¼ˆä»Seleniumè·å–ï¼‰
            if 'headers' in self.auth_package:
                self.session.headers.update(self.auth_package['headers'])
                logger.info("âœ… å·²è®¾ç½®è®¤è¯headers")
                logger.debug(f"Headers keys: {list(self.auth_package['headers'].keys())}")
            else:
                logger.warning("âš ï¸ è®¤è¯åŒ…ä¸­ç¼ºå°‘headers")
            
            # 2. è®¾ç½®è®¤è¯cookiesï¼ˆä»Seleniumè·å–ï¼‰
            if 'cookies' in self.auth_package:
                self.session.cookies.update(self.auth_package['cookies'])
                logger.info("âœ… å·²è®¾ç½®è®¤è¯cookies")
                logger.debug(f"Cookies keys: {list(self.auth_package['cookies'].keys())}")
            else:
                logger.warning("âš ï¸ è®¤è¯åŒ…ä¸­ç¼ºå°‘cookies")
            
            # 3. è®¾ç½®å›ºå®šè¯·æ±‚å¤´ï¼ˆæ ¹æ®cURLåˆ†æï¼‰
            fixed_headers = {
                'Accept': '*/*',
                'Accept-Language': 'zh-CN,zh;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Sec-Fetch-Dest': 'empty',
                'Sec-Fetch-Mode': 'cors',
                'Sec-Fetch-Site': 'same-origin',
                'Sec-Ch-Ua': '"Not;A=Brand";v="99", "Google Chrome";v="139", "Chromium";v="139"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"macOS"',
                'Priority': 'u=1, i',
                'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
            }
            
            self.session.headers.update(fixed_headers)
            logger.info("âœ… å·²è®¾ç½®å›ºå®šè¯·æ±‚å¤´")
            
            # 4. éªŒè¯å…³é”®è®¤è¯ä¿¡æ¯
            required_auth_keys = ['pcuss', 'user']
            missing_keys = [key for key in required_auth_keys if key not in self.session.headers]
            if missing_keys:
                logger.warning(f"âš ï¸ ç¼ºå°‘å…³é”®è®¤è¯ä¿¡æ¯: {missing_keys}")
            else:
                logger.info("âœ… è®¤è¯ä¿¡æ¯å®Œæ•´")
            
            logger.info("âœ… Sessionè®¾ç½®å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ Sessionè®¾ç½®å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
    
    def fetch_bond_documents(self, bond_code: str, bond_short_name: str = "") -> List[Dict]:
        """
        è·å–æŒ‡å®šå€ºåˆ¸çš„æ‰€æœ‰æ–‡æ¡£
        æŒ‰ç…§READMEè¦æ±‚ç²¾ç¡®æ„é€ APIè¯·æ±‚
        
        Args:
            bond_code: å€ºåˆ¸ä»£ç 
            bond_short_name: å€ºåˆ¸ç®€ç§°ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            List[Dict]: æ–‡æ¡£ä¿¡æ¯åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸ” å¼€å§‹è·å–å€ºåˆ¸æ–‡æ¡£: {bond_code}")
            
            # APIé…ç½®
            api_url = f"{BASE_URL}/finchinaAPP/v1/finchina-search/v1/getF9NoticeList"
            documents = []
            skip = 0
            size = 50
            page = 1
            
            logger.info(f"ğŸ“¡ API URL: {api_url}")
            
            while True:
                logger.debug(f"ğŸ“„ è·å–ç¬¬ {page} é¡µæ•°æ® (skip={skip}, size={size})")
                
                # 1. åŠ¨æ€æ„é€ Referer Headerï¼ˆå”¯ä¸€éœ€è¦åœ¨å¾ªç¯ä¸­æ›´æ–°çš„Headerï¼‰
                dynamic_referer = f"https://www.qyyjt.cn/detail/bond/notice?code={bond_code}&type=co"
                self.session.headers['Referer'] = dynamic_referer
                logger.debug(f"ğŸ”— Referer: {dynamic_referer}")
                
                # 2. æ„é€ è¯·æ±‚ä½“Payloadï¼ˆform-dataæ ¼å¼ï¼‰
                payload = self._construct_payload(bond_code, skip, size)
                logger.debug(f"ğŸ“¦ Payload: {payload}")
                
                # 3. å‘é€POSTè¯·æ±‚
                try:
                    response = self.session.post(
                        api_url, 
                        data=payload,  # ä½¿ç”¨dataå‚æ•°å‘é€form-data
                        timeout=REQUEST_TIMEOUT
                    )
                    
                    logger.debug(f"ğŸ“Š å“åº”çŠ¶æ€ç : {response.status_code}")
                    
                except requests.exceptions.RequestException as e:
                    logger.error(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
                    break
                
                # 4. æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status_code != 200:
                    logger.error(f"âŒ APIè¯·æ±‚å¤±è´¥: {response.status_code}")
                    logger.error(f"å“åº”å†…å®¹: {response.text[:200]}...")
                    break
                
                # 5. è§£æJSONå“åº”
                try:
                    data = response.json()
                    logger.debug(f"ğŸ“‹ APIå“åº”ç»“æ„: {list(data.keys())}")
                    
                    # æ£€æŸ¥APIè¿”å›ç 
                    if data.get('returncode') != 0:
                        error_msg = data.get('message', 'Unknown error')
                        logger.warning(f"âš ï¸ APIè¿”å›é”™è¯¯: {error_msg}")
                        break
                    
                    # 6. è·å–æ–‡æ¡£åˆ—è¡¨
                    items = data.get('data', {}).get('data', [])
                    if not items:
                        logger.info(f"ğŸ“­ ç¬¬ {page} é¡µæ— æ•°æ®ï¼Œåœæ­¢ç¿»é¡µ")
                        break
                    
                    logger.info(f"ğŸ“„ ç¬¬ {page} é¡µè·å–åˆ° {len(items)} æ¡åŸå§‹æ•°æ®")
                    
                    # 7. è§£ææ–‡æ¡£
                    page_documents = self._parse_documents(items, bond_code, bond_short_name)
                    documents.extend(page_documents)
                    
                    logger.info(f"âœ… ç¬¬ {page} é¡µè§£æå‡º {len(page_documents)} ä¸ªæ–‡æ¡£")
                    
                    # 8. æ£€æŸ¥æ˜¯å¦åˆ°è¾¾æœ€åä¸€é¡µ
                    if len(items) < size:
                        logger.info("ğŸ å·²åˆ°è¾¾æœ€åä¸€é¡µï¼ˆè¿”å›æ•°æ®å°‘äºè¯·æ±‚æ•°é‡ï¼‰")
                        break
                    
                    # 9. å‡†å¤‡ä¸‹ä¸€é¡µ
                    skip += size
                    page += 1
                    
                    # 10. éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                    delay = random.uniform(1, 3)
                    logger.debug(f"â³ å»¶è¿Ÿ {delay:.2f} ç§’")
                    time.sleep(delay)
                    
                except json.JSONDecodeError as e:
                    logger.error(f"âŒ JSONè§£æå¤±è´¥: {e}")
                    logger.error(f"å“åº”å†…å®¹: {response.text[:200]}...")
                    break
                except Exception as e:
                    logger.error(f"âŒ è§£æAPIå“åº”å¤±è´¥: {e}")
                    break
            
            logger.info(f"ğŸ‰ å€ºåˆ¸ {bond_code} å…±è·å–åˆ° {len(documents)} ä¸ªæ–‡æ¡£")
            return documents
            
        except Exception as e:
            logger.error(f"âŒ è·å–å€ºåˆ¸æ–‡æ¡£å¤±è´¥ {bond_code}: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return []
    
    def _construct_payload(self, bond_code: str, skip: int, size: int) -> Dict:
        """
        æ„é€ APIè¯·æ±‚ä½“Payload
        æŒ‰ç…§READMEè¦æ±‚ä½¿ç”¨form-dataæ ¼å¼
        
        Args:
            bond_code: å€ºåˆ¸ä»£ç 
            skip: åˆ†é¡µåç§»é‡
            size: æ¯é¡µå¤§å°
            
        Returns:
            Dict: è¯·æ±‚ä½“æ•°æ®
        """
        # åŠ¨æ€å­—æ®µ
        dynamic_fields = {
            'code': bond_code,
            'skip': skip
        }
        
        # å›ºå®šå­—æ®µï¼ˆæ ¹æ®cURLåˆ†æï¼‰
        fixed_fields = {
            'type': 'co',
            'size': size,
            'tab': 'notice_bond_coRelated'
        }
        
        # åˆå¹¶å­—æ®µ
        payload = {**fixed_fields, **dynamic_fields}
        
        logger.debug(f"ğŸ”§ æ„é€ Payload: åŠ¨æ€å­—æ®µ={dynamic_fields}, å›ºå®šå­—æ®µ={fixed_fields}")
        
        return payload
    
    def _parse_documents(self, items: List[Dict], bond_code: str, bond_short_name: str) -> List[Dict]:
        """
        è§£ææ–‡æ¡£æ•°æ®
        
        Args:
            items: APIè¿”å›çš„æ–‡æ¡£åˆ—è¡¨
            bond_code: å€ºåˆ¸ä»£ç 
            bond_short_name: å€ºåˆ¸ç®€ç§°
            
        Returns:
            List[Dict]: è§£æåçš„æ–‡æ¡£åˆ—è¡¨
        """
        documents = []
        
        for item in items:
            try:
                # æå–åŸºæœ¬ä¿¡æ¯
                title = item.get('title', '')
                if not title:
                    continue
                
                # æå–ä¸‹è½½é“¾æ¥
                download_url = self._extract_download_url(item)
                if not download_url:
                    logger.warning(f"æ–‡æ¡£æ— ä¸‹è½½é“¾æ¥: {title}")
                    continue
                
                # æå–æ–‡æ¡£ç±»å‹
                document_type = self._classify_document_type(title)
                
                # æå–å‘å¸ƒæ—¥æœŸ
                publication_date = self._format_date(item.get('date', ''))
                
                # æå–æ–‡ä»¶å¤§å°
                file_size = self._extract_file_size(item)
                
                # æ„é€ æ–‡æ¡£ä¿¡æ¯
                document = {
                    'bond_code': bond_code,
                    'bond_short_name': bond_short_name or bond_code,
                    'bond_full_name': item.get('companyName', bond_short_name or bond_code),
                    'document_title': title,
                    'document_type': document_type,
                    'download_url': download_url,
                    'file_size': file_size,
                    'publication_date': publication_date,
                    'province': '',  # å°†åœ¨main.pyä¸­è¡¥å……
                    'city': ''       # å°†åœ¨main.pyä¸­è¡¥å……
                }
                
                documents.append(document)
                
            except Exception as e:
                logger.warning(f"è§£ææ–‡æ¡£å¤±è´¥: {e}")
                continue
        
        return documents
    
    def _extract_download_url(self, item: Dict) -> Optional[str]:
        """æå–ä¸‹è½½é“¾æ¥"""
        try:
            # å°è¯•å¤šç§å¯èƒ½çš„å­—æ®µ
            url_fields = ['downloadUrl', 'url', 'fileUrl', 'link']
            
            for field in url_fields:
                if field in item and item[field]:
                    url = item[field]
                    # ç¡®ä¿æ˜¯å®Œæ•´çš„URL
                    if url.startswith('http'):
                        return url
                    elif url.startswith('/'):
                        return f"{BASE_URL}{url}"
            
            return None
            
        except Exception as e:
            logger.warning(f"æå–ä¸‹è½½é“¾æ¥å¤±è´¥: {e}")
            return None
    
    def _classify_document_type(self, title: str) -> str:
        """æ ¹æ®æ ‡é¢˜åˆ†ç±»æ–‡æ¡£ç±»å‹"""
        title_lower = title.lower()
        
        # å®šä¹‰å…³é”®è¯æ˜ å°„
        keywords = {
            'å‹Ÿé›†è¯´æ˜ä¹¦': ['å‹Ÿé›†è¯´æ˜ä¹¦', 'prospectus'],
            'å‘è¡Œå…¬å‘Š': ['å‘è¡Œå…¬å‘Š', 'issue announcement'],
            'è¯„çº§æŠ¥å‘Š': ['è¯„çº§æŠ¥å‘Š', 'rating report'],
            'è´¢åŠ¡æŠ¥å‘Š': ['è´¢åŠ¡æŠ¥å‘Š', 'financial report', 'å¹´æŠ¥', 'åŠå¹´æŠ¥', 'å­£æŠ¥'],
            'å®¡è®¡æŠ¥å‘Š': ['å®¡è®¡æŠ¥å‘Š', 'audit report'],
            'æ³•å¾‹æ„è§ä¹¦': ['æ³•å¾‹æ„è§ä¹¦', 'legal opinion'],
            'æ‹…ä¿å‡½': ['æ‹…ä¿å‡½', 'guarantee'],
            'å…¶ä»–': []
        }
        
        for doc_type, keywords_list in keywords.items():
            if doc_type == 'å…¶ä»–':
                continue
            for keyword in keywords_list:
                if keyword in title_lower:
                    return doc_type
        
        return 'å…¶ä»–'
    
    def _format_date(self, date_str: str) -> str:
        """æ ¼å¼åŒ–æ—¥æœŸ"""
        try:
            if not date_str:
                return ''
            
            # å¦‚æœæ˜¯æ—¶é—´æˆ³
            if date_str.isdigit() and len(date_str) >= 8:
                if len(date_str) >= 10:
                    # æ¯«ç§’æ—¶é—´æˆ³
                    timestamp = int(date_str[:10])
                else:
                    # ç§’æ—¶é—´æˆ³
                    timestamp = int(date_str)
                
                from datetime import datetime
                return datetime.fromtimestamp(timestamp).strftime('%Y-%m-%d')
            
            # å¦‚æœå·²ç»æ˜¯æ—¥æœŸæ ¼å¼
            if len(date_str) >= 8:
                return date_str[:8] if len(date_str) >= 8 else date_str
            
            return date_str
            
        except Exception as e:
            logger.warning(f"æ ¼å¼åŒ–æ—¥æœŸå¤±è´¥: {e}")
            return date_str
    
    def _extract_file_size(self, item: Dict) -> str:
        """æå–æ–‡ä»¶å¤§å°"""
        try:
            size_fields = ['fileSize', 'size', 'file_size']
            
            for field in size_fields:
                if field in item and item[field]:
                    return str(item[field])
            
            return ''
            
        except Exception as e:
            logger.warning(f"æå–æ–‡ä»¶å¤§å°å¤±è´¥: {e}")
            return ''
    
    def fetch_multiple_bonds(self, bond_codes: List[str], bond_names: List[str] = None) -> Dict[str, List[Dict]]:
        """
        æ‰¹é‡è·å–å¤šä¸ªå€ºåˆ¸çš„æ–‡æ¡£
        æŒ‰ç…§READMEè¦æ±‚éå†codeåˆ—è¡¨è¿›è¡Œé«˜æ•ˆçˆ¬å–
        
        Args:
            bond_codes: å€ºåˆ¸ä»£ç åˆ—è¡¨
            bond_names: å€ºåˆ¸ç®€ç§°åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Dict[str, List[Dict]]: æ¯ä¸ªå€ºåˆ¸çš„æ–‡æ¡£åˆ—è¡¨
        """
        try:
            logger.info(f"ğŸš€ å¼€å§‹æ‰¹é‡è·å– {len(bond_codes)} ä¸ªå€ºåˆ¸çš„æ–‡æ¡£")
            logger.info(f"ğŸ“‹ å€ºåˆ¸ä»£ç åˆ—è¡¨: {bond_codes}")
            
            results = {}
            success_count = 0
            error_count = 0
            
            for i, bond_code in enumerate(bond_codes):
                bond_name = bond_names[i] if bond_names and i < len(bond_names) else bond_code
                
                logger.info(f"ğŸ“Š å¤„ç†ç¬¬ {i+1}/{len(bond_codes)} ä¸ªå€ºåˆ¸: {bond_name} ({bond_code})")
                
                try:
                    # éªŒè¯APIè¯·æ±‚æ„é€ 
                    if not self._validate_api_request(bond_code):
                        logger.warning(f"âš ï¸ APIè¯·æ±‚éªŒè¯å¤±è´¥: {bond_code}")
                        results[bond_code] = []
                        error_count += 1
                        continue
                    
                    # è·å–æ–‡æ¡£
                    documents = self.fetch_bond_documents(bond_code, bond_name)
                    results[bond_code] = documents
                    
                    if documents:
                        success_count += 1
                        logger.info(f"âœ… {bond_name}: {len(documents)} ä¸ªæ–‡æ¡£")
                    else:
                        logger.warning(f"âš ï¸ {bond_name}: æœªè·å–åˆ°æ–‡æ¡£")
                        error_count += 1
                    
                except Exception as e:
                    logger.error(f"âŒ å¤„ç†å€ºåˆ¸å¤±è´¥ {bond_name}: {e}")
                    results[bond_code] = []
                    error_count += 1
                    continue
                
                # éšæœºå»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                delay = random.uniform(2, 4)
                logger.debug(f"â³ å»¶è¿Ÿ {delay:.2f} ç§’")
                time.sleep(delay)
            
            total_documents = sum(len(docs) for docs in results.values())
            logger.info(f"ğŸ‰ æ‰¹é‡è·å–å®Œæˆï¼")
            logger.info(f"ğŸ“ˆ ç»Ÿè®¡: æˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {error_count} ä¸ªï¼Œå…± {total_documents} ä¸ªæ–‡æ¡£")
            
            return results
            
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡è·å–å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return {}
    
    def _validate_api_request(self, bond_code: str) -> bool:
        """
        éªŒè¯APIè¯·æ±‚æ„é€ æ˜¯å¦æ­£ç¡®
        
        Args:
            bond_code: å€ºåˆ¸ä»£ç 
            
        Returns:
            bool: éªŒè¯æ˜¯å¦é€šè¿‡
        """
        try:
            # æ£€æŸ¥è®¤è¯ä¿¡æ¯
            required_headers = ['pcuss', 'user']
            missing_headers = [h for h in required_headers if h not in self.session.headers]
            if missing_headers:
                logger.warning(f"âš ï¸ ç¼ºå°‘è®¤è¯headers: {missing_headers}")
                return False
            
            # æ£€æŸ¥cookies
            if not self.session.cookies:
                logger.warning("âš ï¸ ç¼ºå°‘è®¤è¯cookies")
                return False
            
            # æ£€æŸ¥API URL
            api_url = f"{BASE_URL}/finchinaAPP/v1/finchina-search/v1/getF9NoticeList"
            if not api_url:
                logger.warning("âš ï¸ API URLæœªé…ç½®")
                return False
            
            # æ£€æŸ¥å€ºåˆ¸ä»£ç 
            if not bond_code:
                logger.warning("âš ï¸ å€ºåˆ¸ä»£ç ä¸ºç©º")
                return False
            
            logger.debug(f"âœ… APIè¯·æ±‚éªŒè¯é€šè¿‡: {bond_code}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ APIè¯·æ±‚éªŒè¯å¤±è´¥: {e}")
            return False


def test_requests_scraper():
    """æµ‹è¯•Requestsçˆ¬å–åŠŸèƒ½"""
    try:
        # æ¨¡æ‹Ÿè®¤è¯åŒ…
        test_auth_package = {
            'headers': {
                'pcuss': 'test_pcuss_token',
                'user': 'test_user_token',
                'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
                'client': 'pc-web;pro'
            },
            'cookies': {
                'HWWAFSESTIME': 'test_time',
                'HWWAFSESID': 'test_id'
            }
        }
        
        # åˆ›å»ºçˆ¬å–å™¨
        scraper = RequestsDataScraper(test_auth_package)
        
        # æµ‹è¯•å•ä¸ªå€ºåˆ¸
        test_codes = ['TEST001', 'TEST002']
        results = scraper.fetch_multiple_bonds(test_codes, ['æµ‹è¯•å€ºåˆ¸1', 'æµ‹è¯•å€ºåˆ¸2'])
        
        logger.info(f"æµ‹è¯•ç»“æœ: {len(results)} ä¸ªå€ºåˆ¸")
        
    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}")


if __name__ == "__main__":
    test_requests_scraper()
