# QYYJTScraper - 企业预警通债券公告爬虫

这是一个针对 **企业预警通 (qyyjt.cn)** 网站的 Python 爬虫项目，旨在自动化地抓取指定债券列表的所有相关公告信息，并将结果持久化存储到 SQLite 数据库中。

该爬虫通过模拟登录获取认证凭据，然后直接与后端 API 交互，实现了高效、稳定的数据采集。同时，它还内置了多账号轮换、速率限制处理、断点续传等高级功能，非常适合进行批量数据采集任务。

## 目录

- [主要功能](#主要功能)
- [项目结构](#项目结构)
- [环境准备与依赖安装](#环境准备与依赖安装)
- [配置指南](#配置指南)
  - [1. 配置账号池 (accounts.json)](#1-配置账号池-accountsjson)
  - [2. 配置待爬取列表 (bonds_list.xlsx)](#2-配置待爬取列表-bonds_listxlsx)
  - [3. (可选) 调整核心配置 (config.py)](#3-可选-调整核心配置-configpy)
- [如何运行](#如何运行)
- [工作流程详解](#工作流程详解)
- [输出结果](#输出结果)
- [注意事项](#注意事项)

## 主要功能

- **自动化模拟登录**: 使用 Selenium 自动完成登录过程，获取必要的 API 访问令牌和 Cookie。
- **多账号池轮换**: 支持配置多个账号，当一个账号达到请求上限或被临时封禁时，程序会自动切换到下一个可用账号。
- **智能速率限制处理**: 能自动识别 API 返回的“请求过于频繁”错误，并将触发该错误的账号暂时移出任务池。
- **断点续传**: 在启动时会检查数据库，自动跳过已经爬取过的债券，避免重复工作。
- **分页数据抓取**: 自动处理公告列表的分页，抓取目标债券的全部历史公告。
- **数据持久化**: 将抓取到的公告标题、发布日期、文件链接、文件大小等信息存入本地 SQLite 数据库，方便后续分析。
- **高度可配置**: 核心参数（如请求延时、每账号请求次数、文件路径等）均可通过 `config.py` 文件进行调整。
- **测试模式**: 内置测试模式开关，方便快速验证程序逻辑是否正常。

## 项目结构

```
QYYJTScraper/
│
├── data/                     # 数据与配置文件目录
│   ├── accounts.json         # 【需手动创建】用于存放登录账号和密码
│   └── bonds_list.xlsx       # 【需手动创建】待爬取的债券简称列表
│
├── src/                      # 源代码目录
│   ├── __init__.py
│   ├── main.py               # 爬虫主程序入口
│   ├── config.py             # 核心配置文件
│   ├── login_handler.py      # 负责模拟登录与获取会话
│   ├── scraper.py            # 负责API请求和数据解析
│   └── database.py           # 负责数据库的初始化与操作
│
└── README.md                 # 项目说明文档
```

## 环境准备与依赖安装

1.  **安装 Python**: 建议使用 Python 3.8 或更高版本。
2.  **安装 Chrome 浏览器**: 本项目使用 Selenium 驱动 Chrome 浏览器进行模拟登录。
3.  **安装依赖库**: 在项目根目录下，通过 pip 安装所有必要的库。

    ```bash
    pip install pandas openpyxl requests selenium webdriver-manager
    ```

## 配置指南

在运行爬虫之前，你需要进行以下三个步骤的配置。

### 1. 配置账号池 (accounts.json)

在项目根目录（与 `src` 文件夹同级）下，手动创建一个名为 `accounts.json` 的文件。该文件用于存储一个或多个企业预警通的登录账号。

文件内容应遵循以下 JSON 格式：

```json
{
  "accounts": [
    {
      "phone": "你的手机号1",
      "password": "你的密码1"
    },
    {
      "phone": "你的手机号2",
      "password": "你的密码2"
    }
  ]
}
```

**提示**: 账号越多，爬虫的抗封禁能力越强。

### 2. 配置待爬取列表 (bonds_list.xlsx)

在项目根目录下，手动创建一个名为 `bonds_list.xlsx` 的 Excel 文件。

-   文件中必须包含一个名为 **`债券简称`** 的列。
-   在该列下方，逐行填入你想要爬取的债券的简称。

| 债券简称         |
| ---------------- |
| 21沪世业MTN001   |
| 23蓉城建工SCP001 |
| 20大连德泰MTN001 |
| ...              |

爬虫会读取此列中的所有内容，并去除重复项和空值。

### 3. 调整核心配置 (config.py)

`src/config.py` 文件包含了爬虫的所有核心配置项，你可以根据需要进行修改。**大部分情况下，你只需要关注前几个配置项。**

```python
# src/config.py

# --- 基础配置 (通常需要检查) ---
# 数据库文件名
DATABASE_NAME = 'qyyjt_data.db' 
# 账号池文件路径
ACCOUNTS_FILE_PATH = 'accounts.json'
# 待爬取债券列表文件路径
BONDS_LIST_PATH = 'bonds_list.xlsx'
# Excel中包含债券名称的列名
BONDS_LIST_COLUMN_NAME = '债券简称'

# --- 性能与反爬配置 (可根据网络情况和风险承受能力调整) ---
# 每个账号连续执行多少次任务后强制切换
REQUESTS_PER_ACCOUNT = 20
# 爬取不同债券之间的随机延迟范围 (秒)
DELAY_BETWEEN_BONDS = (2, 5)
# 爬取同一债券不同公告页面之间的随机延迟范围 (秒)
DELAY_BETWEEN_PAGES = (0.5, 1.5)

# --- 测试模式 ---
# 是否开启测试模式 (True: 仅爬取少量数据; False: 全量爬取)
TEST_MODE = True
# 测试模式下处理的债券数量
TEST_MODE_BOND_COUNT = 3
```

## 如何运行

1.  确保你已经完成了上述所有配置步骤。
2.  打开终端或命令行，切换到项目根目录 `QYYJTScraper/`。
3.  运行主程序 `main.py`。推荐使用模块化方式运行：

    ```bash
    python -m src.main
    ```

4.  程序启动后，你将看到如下日志输出：
    -   加载账号和债券列表的信息。
    -   初始化数据库。
    -   如果数据库中已有数据，会提示跳过已爬取的债券。
    -   程序会启动一个 Chrome 浏览器窗口，自动进行登录操作。登录成功后，浏览器会自动关闭。
    -   之后，程序将在命令行中持续输出爬取进度、保存的数据条数、账号切换等信息。

## 工作流程详解

1.  **初始化**: `main.py` 启动，加载 `accounts.json` 和 `bonds_list.xlsx` 中的数据，并初始化数据库。
2.  **断点检查**: 通过 `database.py` 查询数据库，获取所有已爬取过的 `search_term`（即债券简称），并从待爬取列表中移除它们。
3.  **获取会话**:
    -   从账号池中选择一个账号。
    -   调用 `login_handler.py`，启动 Selenium 浏览器。
    -   模拟用户输入账号密码、点击登录。
    -   登录成功后，从浏览器的 `localStorage` 中提取 `s_tk` (Token) 和 `u_info` (用户ID)。
    -   关闭浏览器，返回包含认证信息的会话字典。
4.  **创建 Scraper 实例**: 使用上一步获取的会话信息，创建一个 `scraper.Scraper` 实例，该实例包含了后续所有 API 请求所需的 `headers` 和 `cookies`。
5.  **循环处理任务**:
    -   从待爬取列表中取出一个债券简称。
    -   调用 `scraper.search_bond()` 方法，请求搜索 API，找到该债券的内部 `code`。
    -   调用 `scraper.get_announcements()` 方法，使用 `code` 请求公告 API，并通过循环分页获取所有公告数据。
    -   **异常处理**:
        -   如果在 API 请求中，`scraper` 检测到 "请求次数过多" 的响应，它会抛出 `RateLimitException`。
        -   `main.py` 捕获此异常，将当前账号从可用池中移除，然后使用下一个账号 **重试同一个债券**。
    -   **数据存储**: 如果成功获取公告，调用 `database.save_announcements()` 将数据存入 SQLite 数据库。数据库对 `file_url` 字段设置了 `UNIQUE` 约束，可自动去重。
6.  **账号轮换**:
    -   每成功处理一个债券，计数器加一。
    -   当一个账号处理的任务数达到 `REQUESTS_PER_ACCOUNT` 设定的阈值时，程序会主动废弃当前会话，并切换到账号池中的下一个账号，以降低被封禁风险。
7.  **任务结束**: 当所有待爬取债券都处理完毕，或所有账号都已失效时，程序结束。

## 输出结果

所有爬取到的数据都存储在项目根目录下的 `qyyjt_data.db` 文件中。这是一个标准的 SQLite 数据库文件，你可以使用任何 SQLite 客户端工具（如 DB Browser for SQLite）打开查看。

数据库中包含一个名为 `announcements` 的表，其结构如下：

| 字段名               | 类型      | 描述                               |
| -------------------- | --------- | ---------------------------------- |
| `id`                 | INTEGER   | 主键，自增                         |
| `search_term`        | TEXT      | 爬取时使用的原始搜索词（债券简称） |
| `bond_name`          | TEXT      | API 返回的规范债券名称             |
| `bond_code`          | TEXT      | 债券在网站内部的唯一代码           |
| `announcement_title` | TEXT      | 公告的完整标题                     |
| `file_url`           | TEXT      | 公告PDF文件的下载链接（唯一）      |
| `file_size`          | TEXT      | 文件大小 (如 "2.13MB")             |
| `publish_date`       | TEXT      | 公告发布日期 (如 "2023-10-26")     |
| `scraped_at`         | TIMESTAMP | 该条记录的爬取时间                 |

## 注意事项

-   **遵守网站规则**: 请合理使用本爬虫，尊重目标网站的 `robots.txt` 协议和服务条款。过于频繁的请求可能导致你的账号或 IP 地址被封禁。
-   **代码维护**: 网站的前端或后端 API 随时可能发生变化，导致爬虫失效。如果遇到问题，可能需要根据新的网络请求更新 `config.py` 中的 URL 和 `scraper.py` 中的解析逻辑。
-   **法律与道德风险**: 本项目仅供学习和技术研究使用。请勿用于任何非法或商业用途。因使用本代码而产生的任何法律后果，由使用者自行承担。